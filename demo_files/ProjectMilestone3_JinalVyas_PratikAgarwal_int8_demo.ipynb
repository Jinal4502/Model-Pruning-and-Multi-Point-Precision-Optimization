{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "NOTE -> The right side python files are the main source files.\n",
        "\n",
        "These .ipynb notebooks are just for demo purpose, they don't include the calculation of \n",
        "top - 5 accuracy because it is time consuming for the time frame of demo.\n",
        "\n",
        "This demo includes running implementation on CPU. The detailed results for both CPU and GPU\n",
        "utilization are attached in the report.\n",
        "\n",
        "For this demo purpose, I am testing inference on one single image and comparing its output classes' probability. In the report I have attached results for Imagenet1k validation set containing 1000 validation images\n",
        "\n",
        "Also, the inference time might differ from the one mentioned in the report, because while giving final runs, I ensured that no background processes are running, which as you can see is not the case currently, the vscode environment, recorder and a few other functionalities are on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B80np2RkpoP_",
        "outputId": "82cc36ae-3ce2-4291-f3d5-69c3db7853be"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.utils.prune as prune\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import time\n",
        "import psutil\n",
        "import os\n",
        "import copy\n",
        "import torch.nn as nn\n",
        "from torchvision.models.resnet import BasicBlock\n",
        "from torch.ao.quantization.fake_quantize import FakeQuantize\n",
        "from torch import Tensor\n",
        "from typing import Any, Callable, List, Optional, Type, Union\n",
        "\n",
        "# 1. Load pretrained ResNet-18\n",
        "model = models.resnet18(pretrained=True)\n",
        "model.eval()\n",
        "\n",
        "# 4. Prepare a dummy input\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "img = Image.open(\"/Users/jjvyas1/Downloads/mla_project/n01443537_goldfish.JPEG\").convert(\"RGB\")  # replace with a real image path\n",
        "input_tensor = transform(img).unsqueeze(0)  # batch dimension"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PsSplzkQBD-K",
        "outputId": "141b323e-2a03-4b94-bc2a-d0858681ecca"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 118,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "id": "XYnj0cekYXDm"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, device_str='cuda'):\n",
        "    global input_tensor\n",
        "    input_tensor = input_tensor\n",
        "    if not (device_str in['cpu', 'cuda']):\n",
        "        raise NotImplementedError(\"`device_str` should be 'cpu' or 'cuda' \")\n",
        "    if device_str == 'cuda':\n",
        "        assert torch.cuda.is_available(), 'Check CUDA is available'\n",
        "    input_batch = input_tensor.unsqueeze(0)[0]\n",
        "    input_batch = input_batch.to(device_str)\n",
        "    model.to(device_str)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(input_batch)\n",
        "\n",
        "    probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
        "    with open(\"/Users/jjvyas1/Downloads/mla_project/imagenet_classes.txt\", \"r\") as f:\n",
        "        categories = [s.strip() for s in f.readlines()]\n",
        "    top5_prob, top5_catid = torch.topk(probabilities, 5)\n",
        "    for i in range(top5_prob.size(0)):\n",
        "        print(categories[top5_catid[i]], top5_prob[i].item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LplrKoW7YYOv",
        "outputId": "a706779f-8ae6-43c7-90df-b3563673c3d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "goldfish 0.8217602968215942\n",
            "axolotl 0.14192184805870056\n",
            "tench 0.01749112270772457\n",
            "sturgeon 0.005148489493876696\n",
            "puffer 0.004762660246342421\n"
          ]
        }
      ],
      "source": [
        "evaluate(model, 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "id": "x2GzQ6tRalmR"
      },
      "outputs": [],
      "source": [
        "def get_model_size(model):\n",
        "    torch.save(model.state_dict(), \"temp.p\")\n",
        "    size_mb = os.path.getsize(\"temp.p\") / 1e6\n",
        "    os.remove(\"temp.p\")\n",
        "    print(f\"Model size: {size_mb:.2f} MB\")\n",
        "    return size_mb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-4vZKaoJmUUu",
        "outputId": "d1ceadfe-6775-4134-deb1-36c18020d9da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model size: 46.83 MB\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "46.828292"
            ]
          },
          "execution_count": 122,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "get_model_size(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {},
      "outputs": [],
      "source": [
        "def measure_inference_time(model, input_size=(1, 3, 224, 224), device='cpu', repeats=50):\n",
        "    model.eval()\n",
        "    dummy_input = torch.randn(input_size).to(device)\n",
        "    model.to(device)\n",
        "\n",
        "    for _ in range(10):\n",
        "        _ = model(dummy_input)\n",
        "\n",
        "    start = time.time()\n",
        "    for _ in range(repeats):\n",
        "        _ = model(dummy_input)\n",
        "    end = time.time()\n",
        "\n",
        "    avg_time = (end - start) / repeats\n",
        "    print(f\"Inference time (avg over {repeats}): {avg_time*1000:.2f} ms\")\n",
        "    return avg_time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Inference time (avg over 50): 15.57 ms\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.015571141242980957"
            ]
          },
          "execution_count": 124,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "measure_inference_time(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here, I am importing custom resnet18 with modules_to_fuse methods implemented for fusing \n",
        "layers for quantization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hlDBN_Hi0Yo7",
        "outputId": "fd6c6c28-28b0-4edf-e605-10418c23bfbf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu1): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (add_relu_FF): FloatFunctional(\n",
              "        (activation_post_process): Identity()\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu1): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (add_relu_FF): FloatFunctional(\n",
              "        (activation_post_process): Identity()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu1): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (add_relu_FF): FloatFunctional(\n",
              "        (activation_post_process): Identity()\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu1): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (add_relu_FF): FloatFunctional(\n",
              "        (activation_post_process): Identity()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu1): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (add_relu_FF): FloatFunctional(\n",
              "        (activation_post_process): Identity()\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu1): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (add_relu_FF): FloatFunctional(\n",
              "        (activation_post_process): Identity()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu1): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (add_relu_FF): FloatFunctional(\n",
              "        (activation_post_process): Identity()\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu1): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (add_relu_FF): FloatFunctional(\n",
              "        (activation_post_process): Identity()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
              "  (quant): QuantStub()\n",
              "  (dequant): DeQuantStub()\n",
              ")"
            ]
          },
          "execution_count": 125,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from Downloads.mla_project.resnet import resnet18\n",
        "# loading the quantized model\n",
        "model = resnet18(pretrained=True)\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "luhzWMjXrWsJ",
        "outputId": "3936f189-8a02-40e3-b388-1ded946d61a4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['conv1', 'bn1', 'relu'],\n",
              " ['layer1.0.conv1', 'layer1.0.bn1', 'layer1.0.relu1'],\n",
              " ['layer1.0.conv2', 'layer1.0.bn2'],\n",
              " ['layer1.1.conv1', 'layer1.1.bn1', 'layer1.1.relu1'],\n",
              " ['layer1.1.conv2', 'layer1.1.bn2'],\n",
              " ['layer2.0.conv1', 'layer2.0.bn1', 'layer2.0.relu1'],\n",
              " ['layer2.0.conv2', 'layer2.0.bn2'],\n",
              " ['layer2.0.downsample.0', 'layer2.0.downsample.1'],\n",
              " ['layer2.1.conv1', 'layer2.1.bn1', 'layer2.1.relu1'],\n",
              " ['layer2.1.conv2', 'layer2.1.bn2'],\n",
              " ['layer3.0.conv1', 'layer3.0.bn1', 'layer3.0.relu1'],\n",
              " ['layer3.0.conv2', 'layer3.0.bn2'],\n",
              " ['layer3.0.downsample.0', 'layer3.0.downsample.1'],\n",
              " ['layer3.1.conv1', 'layer3.1.bn1', 'layer3.1.relu1'],\n",
              " ['layer3.1.conv2', 'layer3.1.bn2'],\n",
              " ['layer4.0.conv1', 'layer4.0.bn1', 'layer4.0.relu1'],\n",
              " ['layer4.0.conv2', 'layer4.0.bn2'],\n",
              " ['layer4.0.downsample.0', 'layer4.0.downsample.1'],\n",
              " ['layer4.1.conv1', 'layer4.1.bn1', 'layer4.1.relu1'],\n",
              " ['layer4.1.conv2', 'layer4.1.bn2']]"
            ]
          },
          "execution_count": 126,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "modules_to_list = model.modules_to_fuse()\n",
        "modules_to_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JjePJzXFrc2N",
        "outputId": "9ff8bb14-c991-454b-bef2-a16835ac02d3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): ConvReLU2d(\n",
              "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
              "    (1): ReLU(inplace=True)\n",
              "  )\n",
              "  (bn1): Identity()\n",
              "  (relu): Identity()\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): ConvReLU2d(\n",
              "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (1): ReLU(inplace=True)\n",
              "      )\n",
              "      (bn1): Identity()\n",
              "      (relu1): Identity()\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (bn2): Identity()\n",
              "      (add_relu_FF): FloatFunctional(\n",
              "        (activation_post_process): Identity()\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): ConvReLU2d(\n",
              "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (1): ReLU(inplace=True)\n",
              "      )\n",
              "      (bn1): Identity()\n",
              "      (relu1): Identity()\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (bn2): Identity()\n",
              "      (add_relu_FF): FloatFunctional(\n",
              "        (activation_post_process): Identity()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): ConvReLU2d(\n",
              "        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "        (1): ReLU(inplace=True)\n",
              "      )\n",
              "      (bn1): Identity()\n",
              "      (relu1): Identity()\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (bn2): Identity()\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))\n",
              "        (1): Identity()\n",
              "      )\n",
              "      (add_relu_FF): FloatFunctional(\n",
              "        (activation_post_process): Identity()\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): ConvReLU2d(\n",
              "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (1): ReLU(inplace=True)\n",
              "      )\n",
              "      (bn1): Identity()\n",
              "      (relu1): Identity()\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (bn2): Identity()\n",
              "      (add_relu_FF): FloatFunctional(\n",
              "        (activation_post_process): Identity()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): ConvReLU2d(\n",
              "        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "        (1): ReLU(inplace=True)\n",
              "      )\n",
              "      (bn1): Identity()\n",
              "      (relu1): Identity()\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (bn2): Identity()\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))\n",
              "        (1): Identity()\n",
              "      )\n",
              "      (add_relu_FF): FloatFunctional(\n",
              "        (activation_post_process): Identity()\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): ConvReLU2d(\n",
              "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (1): ReLU(inplace=True)\n",
              "      )\n",
              "      (bn1): Identity()\n",
              "      (relu1): Identity()\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (bn2): Identity()\n",
              "      (add_relu_FF): FloatFunctional(\n",
              "        (activation_post_process): Identity()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): ConvReLU2d(\n",
              "        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "        (1): ReLU(inplace=True)\n",
              "      )\n",
              "      (bn1): Identity()\n",
              "      (relu1): Identity()\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (bn2): Identity()\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2))\n",
              "        (1): Identity()\n",
              "      )\n",
              "      (add_relu_FF): FloatFunctional(\n",
              "        (activation_post_process): Identity()\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): ConvReLU2d(\n",
              "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (1): ReLU(inplace=True)\n",
              "      )\n",
              "      (bn1): Identity()\n",
              "      (relu1): Identity()\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (bn2): Identity()\n",
              "      (add_relu_FF): FloatFunctional(\n",
              "        (activation_post_process): Identity()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
              "  (quant): QuantStub()\n",
              "  (dequant): DeQuantStub()\n",
              ")"
            ]
          },
          "execution_count": 127,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.eval()\n",
        "fused_model = torch.ao.quantization.fuse_modules(model, modules_to_list)\n",
        "fused_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wwBw7UYrTfg",
        "outputId": "33a77b4f-887e-4b16-c56f-bf469c34552a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): ConvReLU2d(\n",
              "    3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3)\n",
              "    (weight_fake_quant): FakeQuantize(\n",
              "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32)\n",
              "      (activation_post_process): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
              "    )\n",
              "    (activation_post_process): FakeQuantize(\n",
              "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32)\n",
              "      (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
              "    )\n",
              "  )\n",
              "  (bn1): Identity()\n",
              "  (relu): Identity()\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): ConvReLU2d(\n",
              "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
              "        (weight_fake_quant): FakeQuantize(\n",
              "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32)\n",
              "          (activation_post_process): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
              "        )\n",
              "        (activation_post_process): FakeQuantize(\n",
              "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32)\n",
              "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
              "        )\n",
              "      )\n",
              "      (bn1): Identity()\n",
              "      (relu1): Identity()\n",
              "      (conv2): Conv2d(\n",
              "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
              "        (weight_fake_quant): FakeQuantize(\n",
              "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32)\n",
              "          (activation_post_process): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
              "        )\n",
              "        (activation_post_process): FakeQuantize(\n",
              "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32)\n",
              "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
              "        )\n",
              "      )\n",
              "      (bn2): Identity()\n",
              "      (add_relu_FF): FloatFunctional(\n",
              "        (activation_post_process): FakeQuantize(\n",
              "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32)\n",
              "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): ConvReLU2d(\n",
              "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
              "        (weight_fake_quant): FakeQuantize(\n",
              "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32)\n",
              "          (activation_post_process): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
              "        )\n",
              "        (activation_post_process): FakeQuantize(\n",
              "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32)\n",
              "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
              "        )\n",
              "      )\n",
              "      (bn1): Identity()\n",
              "      (relu1): Identity()\n",
              "      (conv2): Conv2d(\n",
              "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
              "        (weight_fake_quant): FakeQuantize(\n",
              "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32)\n",
              "          (activation_post_process): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
              "        )\n",
              "        (activation_post_process): FakeQuantize(\n",
              "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32)\n",
              "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
              "        )\n",
              "      )\n",
              "      (bn2): Identity()\n",
              "      (add_relu_FF): FloatFunctional(\n",
              "        (activation_post_process): FakeQuantize(\n",
              "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32)\n",
              "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): ConvReLU2d(\n",
              "        64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)\n",
              "        (weight_fake_quant): FakeQuantize(\n",
              "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32)\n",
              "          (activation_post_process): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
              "        )\n",
              "        (activation_post_process): FakeQuantize(\n",
              "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32)\n",
              "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
              "        )\n",
              "      )\n",
              "      (bn1): Identity()\n",
              "      (relu1): Identity()\n",
              "      (conv2): Conv2d(\n",
              "        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
              "        (weight_fake_quant): FakeQuantize(\n",
              "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32)\n",
              "          (activation_post_process): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
              "        )\n",
              "        (activation_post_process): FakeQuantize(\n",
              "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32)\n",
              "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
              "        )\n",
              "      )\n",
              "      (bn2): Identity()\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(\n",
              "          64, 128, kernel_size=(1, 1), stride=(2, 2)\n",
              "          (weight_fake_quant): FakeQuantize(\n",
              "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32)\n",
              "            (activation_post_process): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
              "          )\n",
              "          (activation_post_process): FakeQuantize(\n",
              "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32)\n",
              "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
              "          )\n",
              "        )\n",
              "        (1): Identity()\n",
              "      )\n",
              "      (add_relu_FF): FloatFunctional(\n",
              "        (activation_post_process): FakeQuantize(\n",
              "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32)\n",
              "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): ConvReLU2d(\n",
              "        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
              "        (weight_fake_quant): FakeQuantize(\n",
              "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32)\n",
              "          (activation_post_process): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
              "        )\n",
              "        (activation_post_process): FakeQuantize(\n",
              "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32)\n",
              "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
              "        )\n",
              "      )\n",
              "      (bn1): Identity()\n",
              "      (relu1): Identity()\n",
              "      (conv2): Conv2d(\n",
              "        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
              "        (weight_fake_quant): FakeQuantize(\n",
              "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32)\n",
              "          (activation_post_process): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
              "        )\n",
              "        (activation_post_process): FakeQuantize(\n",
              "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32)\n",
              "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
              "        )\n",
              "      )\n",
              "      (bn2): Identity()\n",
              "      (add_relu_FF): FloatFunctional(\n",
              "        (activation_post_process): FakeQuantize(\n",
              "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32)\n",
              "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): ConvReLU2d(\n",
              "        128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)\n",
              "        (weight_fake_quant): FakeQuantize(\n",
              "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32)\n",
              "          (activation_post_process): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
              "        )\n",
              "        (activation_post_process): FakeQuantize(\n",
              "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32)\n",
              "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
              "        )\n",
              "      )\n",
              "      (bn1): Identity()\n",
              "      (relu1): Identity()\n",
              "      (conv2): Conv2d(\n",
              "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
              "        (weight_fake_quant): FakeQuantize(\n",
              "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32)\n",
              "          (activation_post_process): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
              "        )\n",
              "        (activation_post_process): FakeQuantize(\n",
              "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32)\n",
              "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
              "        )\n",
              "      )\n",
              "      (bn2): Identity()\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(\n",
              "          128, 256, kernel_size=(1, 1), stride=(2, 2)\n",
              "          (weight_fake_quant): FakeQuantize(\n",
              "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32)\n",
              "            (activation_post_process): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
              "          )\n",
              "          (activation_post_process): FakeQuantize(\n",
              "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32)\n",
              "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
              "          )\n",
              "        )\n",
              "        (1): Identity()\n",
              "      )\n",
              "      (add_relu_FF): FloatFunctional(\n",
              "        (activation_post_process): FakeQuantize(\n",
              "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32)\n",
              "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): ConvReLU2d(\n",
              "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
              "        (weight_fake_quant): FakeQuantize(\n",
              "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32)\n",
              "          (activation_post_process): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
              "        )\n",
              "        (activation_post_process): FakeQuantize(\n",
              "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32)\n",
              "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
              "        )\n",
              "      )\n",
              "      (bn1): Identity()\n",
              "      (relu1): Identity()\n",
              "      (conv2): Conv2d(\n",
              "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
              "        (weight_fake_quant): FakeQuantize(\n",
              "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32)\n",
              "          (activation_post_process): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
              "        )\n",
              "        (activation_post_process): FakeQuantize(\n",
              "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32)\n",
              "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
              "        )\n",
              "      )\n",
              "      (bn2): Identity()\n",
              "      (add_relu_FF): FloatFunctional(\n",
              "        (activation_post_process): FakeQuantize(\n",
              "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32)\n",
              "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): ConvReLU2d(\n",
              "        256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)\n",
              "        (weight_fake_quant): FakeQuantize(\n",
              "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32)\n",
              "          (activation_post_process): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
              "        )\n",
              "        (activation_post_process): FakeQuantize(\n",
              "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32)\n",
              "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
              "        )\n",
              "      )\n",
              "      (bn1): Identity()\n",
              "      (relu1): Identity()\n",
              "      (conv2): Conv2d(\n",
              "        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
              "        (weight_fake_quant): FakeQuantize(\n",
              "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32)\n",
              "          (activation_post_process): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
              "        )\n",
              "        (activation_post_process): FakeQuantize(\n",
              "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32)\n",
              "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
              "        )\n",
              "      )\n",
              "      (bn2): Identity()\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(\n",
              "          256, 512, kernel_size=(1, 1), stride=(2, 2)\n",
              "          (weight_fake_quant): FakeQuantize(\n",
              "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32)\n",
              "            (activation_post_process): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
              "          )\n",
              "          (activation_post_process): FakeQuantize(\n",
              "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32)\n",
              "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
              "          )\n",
              "        )\n",
              "        (1): Identity()\n",
              "      )\n",
              "      (add_relu_FF): FloatFunctional(\n",
              "        (activation_post_process): FakeQuantize(\n",
              "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32)\n",
              "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): ConvReLU2d(\n",
              "        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
              "        (weight_fake_quant): FakeQuantize(\n",
              "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32)\n",
              "          (activation_post_process): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
              "        )\n",
              "        (activation_post_process): FakeQuantize(\n",
              "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32)\n",
              "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
              "        )\n",
              "      )\n",
              "      (bn1): Identity()\n",
              "      (relu1): Identity()\n",
              "      (conv2): Conv2d(\n",
              "        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
              "        (weight_fake_quant): FakeQuantize(\n",
              "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32)\n",
              "          (activation_post_process): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
              "        )\n",
              "        (activation_post_process): FakeQuantize(\n",
              "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32)\n",
              "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
              "        )\n",
              "      )\n",
              "      (bn2): Identity()\n",
              "      (add_relu_FF): FloatFunctional(\n",
              "        (activation_post_process): FakeQuantize(\n",
              "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32)\n",
              "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(\n",
              "    in_features=512, out_features=1000, bias=True\n",
              "    (weight_fake_quant): FakeQuantize(\n",
              "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32)\n",
              "      (activation_post_process): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
              "    )\n",
              "    (activation_post_process): FakeQuantize(\n",
              "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32)\n",
              "      (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
              "    )\n",
              "  )\n",
              "  (quant): QuantStub(\n",
              "    (activation_post_process): FakeQuantize(\n",
              "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32)\n",
              "      (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
              "    )\n",
              "  )\n",
              "  (dequant): DeQuantStub()\n",
              ")"
            ]
          },
          "execution_count": 128,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from torch.ao.quantization.fake_quantize import FakeQuantize\n",
        "activation_qconfig = FakeQuantize.with_args(\n",
        "    observer=torch.ao.quantization.observer.HistogramObserver.with_args(\n",
        "        quant_min=0,\n",
        "        quant_max=255,\n",
        "        dtype=torch.quint8,\n",
        "        qscheme=torch.per_tensor_affine,\n",
        "    )\n",
        ")\n",
        "\n",
        "weight_qconfig = FakeQuantize.with_args(\n",
        "    observer=torch.ao.quantization.observer.PerChannelMinMaxObserver.with_args(\n",
        "        quant_min=-128,\n",
        "        quant_max=127,\n",
        "        dtype=torch.qint8,\n",
        "        qscheme=torch.per_channel_symmetric,\n",
        "    )\n",
        ")\n",
        "\n",
        "qconfig = torch.quantization.QConfig(activation=activation_qconfig,\n",
        "                                      weight=weight_qconfig)\n",
        "fused_model.qconfig = qconfig\n",
        "\n",
        "fused_model.train()\n",
        "fake_quant_model = torch.ao.quantization.prepare_qat(fused_model)\n",
        "fake_quant_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YguvooU2rl7n",
        "outputId": "379f2bbf-5085-434c-8fd1-daed6c501742"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Fake quant - PTQ\n",
            "goldfish 0.8090365529060364\n",
            "axolotl 0.13504262268543243\n",
            "tench 0.022541021928191185\n",
            "sturgeon 0.00920926034450531\n",
            "puffer 0.0070404428988695145\n",
            "\n",
            "Fake quant - post-PTQ\n",
            "goldfish 0.8090365529060364\n",
            "axolotl 0.13504262268543243\n",
            "tench 0.022541021928191185\n",
            "sturgeon 0.00920926034450531\n",
            "puffer 0.0070404428988695145\n",
            "\n",
            "Converted model\n",
            "goldfish 0.7802646160125732\n",
            "axolotl 0.15577368438243866\n",
            "tench 0.026001403108239174\n",
            "sturgeon 0.011617783457040787\n",
            "puffer 0.006790062412619591\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nFake quant - PTQ\")\n",
        "evaluate(fake_quant_model, 'cpu')\n",
        "\n",
        "fake_quant_model.apply(torch.ao.quantization.fake_quantize.disable_observer)\n",
        "\n",
        "print(\"\\nFake quant - post-PTQ\")\n",
        "evaluate(fake_quant_model, 'cpu')\n",
        "\n",
        "\n",
        "# Step 5: convert (true int8 model)\n",
        "torch.backends.quantized.engine = 'qnnpack'\n",
        "converted_model = torch.ao.quantization.convert(fake_quant_model)\n",
        "\n",
        "print(\"\\nConverted model\")\n",
        "evaluate(converted_model, 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-CpaS6IUax6K",
        "outputId": "022dbf6c-f337-488a-a3b6-de4fba04f479"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model size: 11.83 MB\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "11.834198"
            ]
          },
          "execution_count": 130,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "get_model_size(converted_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Inference time (avg over 50): 19.48 ms\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.019482417106628416"
            ]
          },
          "execution_count": 131,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "measure_inference_time(converted_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4H02_4vL3B4n",
        "outputId": "c1f1ace4-c54b-436a-d0ec-da9480b67eab"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): QuantizedConvReLU2d(3, 64, kernel_size=(7, 7), stride=(2, 2), scale=0.00305533641949296, zero_point=0, padding=(3, 3))\n",
              "  (bn1): Identity()\n",
              "  (relu): Identity()\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): QuantizedConvReLU2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.002219036454334855, zero_point=0, padding=(1, 1))\n",
              "      (bn1): Identity()\n",
              "      (relu1): Identity()\n",
              "      (conv2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.013336235657334328, zero_point=127, padding=(1, 1))\n",
              "      (bn2): Identity()\n",
              "      (add_relu_FF): QFunctional(\n",
              "        scale=0.007735367398709059, zero_point=0\n",
              "        (activation_post_process): Identity()\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): QuantizedConvReLU2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.004394977353513241, zero_point=0, padding=(1, 1))\n",
              "      (bn1): Identity()\n",
              "      (relu1): Identity()\n",
              "      (conv2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.015967005863785744, zero_point=144, padding=(1, 1))\n",
              "      (bn2): Identity()\n",
              "      (add_relu_FF): QFunctional(\n",
              "        scale=0.011056358925998211, zero_point=0\n",
              "        (activation_post_process): Identity()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): QuantizedConvReLU2d(64, 128, kernel_size=(3, 3), stride=(2, 2), scale=0.005023206118494272, zero_point=0, padding=(1, 1))\n",
              "      (bn1): Identity()\n",
              "      (relu1): Identity()\n",
              "      (conv2): QuantizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.017707210034132004, zero_point=90, padding=(1, 1))\n",
              "      (bn2): Identity()\n",
              "      (downsample): Sequential(\n",
              "        (0): QuantizedConv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), scale=0.00866786204278469, zero_point=112)\n",
              "        (1): Identity()\n",
              "      )\n",
              "      (add_relu_FF): QFunctional(\n",
              "        scale=0.009186672046780586, zero_point=0\n",
              "        (activation_post_process): Identity()\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): QuantizedConvReLU2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.005754124838858843, zero_point=0, padding=(1, 1))\n",
              "      (bn1): Identity()\n",
              "      (relu1): Identity()\n",
              "      (conv2): QuantizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.016820408403873444, zero_point=135, padding=(1, 1))\n",
              "      (bn2): Identity()\n",
              "      (add_relu_FF): QFunctional(\n",
              "        scale=0.012269865721464157, zero_point=0\n",
              "        (activation_post_process): Identity()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): QuantizedConvReLU2d(128, 256, kernel_size=(3, 3), stride=(2, 2), scale=0.00758524751290679, zero_point=0, padding=(1, 1))\n",
              "      (bn1): Identity()\n",
              "      (relu1): Identity()\n",
              "      (conv2): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.021688444539904594, zero_point=83, padding=(1, 1))\n",
              "      (bn2): Identity()\n",
              "      (downsample): Sequential(\n",
              "        (0): QuantizedConv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), scale=0.00595097616314888, zero_point=180)\n",
              "        (1): Identity()\n",
              "      )\n",
              "      (add_relu_FF): QFunctional(\n",
              "        scale=0.01135631836950779, zero_point=0\n",
              "        (activation_post_process): Identity()\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): QuantizedConvReLU2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.005394933745265007, zero_point=0, padding=(1, 1))\n",
              "      (bn1): Identity()\n",
              "      (relu1): Identity()\n",
              "      (conv2): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.016411039978265762, zero_point=152, padding=(1, 1))\n",
              "      (bn2): Identity()\n",
              "      (add_relu_FF): QFunctional(\n",
              "        scale=0.007974398322403431, zero_point=0\n",
              "        (activation_post_process): Identity()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): QuantizedConvReLU2d(256, 512, kernel_size=(3, 3), stride=(2, 2), scale=0.003923657350242138, zero_point=0, padding=(1, 1))\n",
              "      (bn1): Identity()\n",
              "      (relu1): Identity()\n",
              "      (conv2): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.01950124464929104, zero_point=121, padding=(1, 1))\n",
              "      (bn2): Identity()\n",
              "      (downsample): Sequential(\n",
              "        (0): QuantizedConv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), scale=0.01462660450488329, zero_point=124)\n",
              "        (1): Identity()\n",
              "      )\n",
              "      (add_relu_FF): QFunctional(\n",
              "        scale=0.0117914117872715, zero_point=0\n",
              "        (activation_post_process): Identity()\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): QuantizedConvReLU2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.005488987546414137, zero_point=0, padding=(1, 1))\n",
              "      (bn1): Identity()\n",
              "      (relu1): Identity()\n",
              "      (conv2): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.10926572233438492, zero_point=84, padding=(1, 1))\n",
              "      (bn2): Identity()\n",
              "      (add_relu_FF): QFunctional(\n",
              "        scale=0.07688441872596741, zero_point=0\n",
              "        (activation_post_process): Identity()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): QuantizedLinear(in_features=512, out_features=1000, scale=0.08951269090175629, zero_point=84, qscheme=torch.per_channel_affine)\n",
              "  (quant): Quantize(scale=tensor([0.0038]), zero_point=tensor([0]), dtype=torch.quint8)\n",
              "  (dequant): DeQuantize()\n",
              ")"
            ]
          },
          "execution_count": 132,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "converted_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "for name, module in converted_model.named_modules():\n",
        "    if name == \"fc\":\n",
        "        weights = module.weight().dequantize().flatten().numpy()\n",
        "        plt.hist(weights, bins=15)\n",
        "        plt.title(f\"Histogram of Weights in {name}\")\n",
        "        plt.xlabel(\"Weight value\")\n",
        "        plt.ylabel(\"Frequency\")\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "id": "oHP2bnCxYruv"
      },
      "outputs": [],
      "source": [
        "def custom_abs(flattened_weights, zero_point, scale):\n",
        "  # Dequantize the weights\n",
        "  dequantized_weights = scale * (flattened_weights.dequantize() - zero_point)\n",
        "\n",
        "  # Calculate the absolute values\n",
        "  absolute_flattened_weights = torch.abs(dequantized_weights)\n",
        "  quantized_absolute_weights = torch.quantize_per_tensor(\n",
        "    absolute_flattened_weights, scale, zero_point, torch.qint8)\n",
        "\n",
        "  return quantized_absolute_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZ4ZYG_T6l26",
        "outputId": "219ef070-b095-4c6b-b010-7d02343cf5cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "QuantizedConvReLU2d(3, 64, kernel_size=(7, 7), stride=(2, 2), scale=0.00305533641949296, zero_point=0, padding=(3, 3))\n",
            "QuantizedConvReLU2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.002219036454334855, zero_point=0, padding=(1, 1))\n",
            "QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.013336235657334328, zero_point=127, padding=(1, 1))\n",
            "QuantizedConvReLU2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.004394977353513241, zero_point=0, padding=(1, 1))\n",
            "QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.015967005863785744, zero_point=144, padding=(1, 1))\n",
            "QuantizedConvReLU2d(64, 128, kernel_size=(3, 3), stride=(2, 2), scale=0.005023206118494272, zero_point=0, padding=(1, 1))\n",
            "QuantizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.017707210034132004, zero_point=90, padding=(1, 1))\n",
            "QuantizedConvReLU2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.005754124838858843, zero_point=0, padding=(1, 1))\n",
            "QuantizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.016820408403873444, zero_point=135, padding=(1, 1))\n",
            "QuantizedConvReLU2d(128, 256, kernel_size=(3, 3), stride=(2, 2), scale=0.00758524751290679, zero_point=0, padding=(1, 1))\n",
            "QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.021688444539904594, zero_point=83, padding=(1, 1))\n",
            "QuantizedConvReLU2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.005394933745265007, zero_point=0, padding=(1, 1))\n",
            "QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.016411039978265762, zero_point=152, padding=(1, 1))\n",
            "QuantizedConvReLU2d(256, 512, kernel_size=(3, 3), stride=(2, 2), scale=0.003923657350242138, zero_point=0, padding=(1, 1))\n",
            "QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.01950124464929104, zero_point=121, padding=(1, 1))\n",
            "QuantizedConvReLU2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.005488987546414137, zero_point=0, padding=(1, 1))\n",
            "QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.10926572233438492, zero_point=84, padding=(1, 1))\n"
          ]
        }
      ],
      "source": [
        "import torch.nn.utils.prune as prune\n",
        "import numpy as np\n",
        "\n",
        "for name, module in converted_model.named_modules():\n",
        "  if \"conv1\" in name or \"conv2\" in name:\n",
        "    print(module)\n",
        "    weight = module.weight()\n",
        "    scale = module.scale\n",
        "    zero_point = module.zero_point\n",
        "\n",
        "    # Perform pruning: Setting a fraction of weights to zero (e.g., 30%)\n",
        "    pruning_amount = 0.3\n",
        "    num_weights = weight.numel()\n",
        "    num_pruned_weights = int(pruning_amount * num_weights)\n",
        "\n",
        "    # Flatten the weights, sort by absolute value, and zero out the smallest\n",
        "    zero_point = max(min(zero_point, 127), -128)\n",
        "    weight = custom_abs(weight, zero_point, scale)\n",
        "    flattened_weights = weight.reshape(-1)\n",
        "    sorted_indices = torch.argsort(flattened_weights)\n",
        "\n",
        "    # # Prune the smallest weights\n",
        "    pruned_weights = flattened_weights.clone()\n",
        "    pruned_weights[sorted_indices[:num_pruned_weights]] = 0\n",
        "\n",
        "    # # Reshape pruned weights back to the original shape\n",
        "    pruned_weights = pruned_weights.view_as(weight)\n",
        "\n",
        "    # print(module.weight())\n",
        "    # print(module.bias())\n",
        "    bias = module.bias()\n",
        "    module.set_weight_bias(pruned_weights, bias)\n",
        "\n",
        "  elif \"fc\" in name:\n",
        "    weight, bias = module._weight_bias()\n",
        "    try:\n",
        "      scale = weight.q_scale()\n",
        "      zero_point = weight.q_zero_point()\n",
        "    except:\n",
        "      scale = weight.q_per_channel_scales()\n",
        "      zero_point = weight.q_per_channel_zero_points()\n",
        "      scale = float(scale.mean())\n",
        "      zero_point = int(zero_point.float().mean().round())\n",
        "\n",
        "    # Perform pruning: Setting a fraction of weights to zero (e.g., 30%)\n",
        "    pruning_amount = 0.1\n",
        "    num_weights = weight.numel()\n",
        "    num_pruned_weights = int(pruning_amount * num_weights)\n",
        "\n",
        "    # Flatten the weights, sort by absolute value, and zero out the smallest\n",
        "    zero_point = max(min(zero_point, 127), -128)\n",
        "    weight = custom_abs(weight, zero_point, scale)\n",
        "    flattened_weights = weight.reshape(-1)\n",
        "    sorted_indices = torch.argsort(flattened_weights)\n",
        "\n",
        "    # # Prune the smallest weights\n",
        "    pruned_weights = flattened_weights.clone()\n",
        "    pruned_weights[sorted_indices[:num_pruned_weights]] = 0\n",
        "\n",
        "    # # Reshape pruned weights back to the original shape\n",
        "    pruned_weights = pruned_weights.view_as(weight)\n",
        "\n",
        "    # print(module.weight())\n",
        "    # print(module.bias())\n",
        "    module.set_weight_bias(pruned_weights, bias)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "id": "4uixqK-MZE5p"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model size: 11.75 MB\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "11.748874"
            ]
          },
          "execution_count": 136,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "get_model_size(converted_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Inference time (avg over 50): 19.53 ms\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.01953183650970459"
            ]
          },
          "execution_count": 137,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "measure_inference_time(converted_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tray 0.00109361601062119\n",
            "velvet 0.00109361601062119\n",
            "tabby 0.00109361601062119\n",
            "great white shark 0.0009999769972637296\n",
            "hammerhead 0.0009999769972637296\n"
          ]
        }
      ],
      "source": [
        "evaluate(converted_model, 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "conv1\n",
            "bn1\n",
            "relu\n",
            "maxpool\n",
            "layer1\n",
            "layer1.0\n",
            "layer1.0.conv1\n",
            "layer1.0.bn1\n",
            "layer1.0.relu1\n",
            "layer1.0.conv2\n",
            "layer1.0.bn2\n",
            "layer1.0.add_relu_FF\n",
            "layer1.0.add_relu_FF.activation_post_process\n",
            "layer1.1\n",
            "layer1.1.conv1\n",
            "layer1.1.bn1\n",
            "layer1.1.relu1\n",
            "layer1.1.conv2\n",
            "layer1.1.bn2\n",
            "layer1.1.add_relu_FF\n",
            "layer1.1.add_relu_FF.activation_post_process\n",
            "layer2\n",
            "layer2.0\n",
            "layer2.0.conv1\n",
            "layer2.0.bn1\n",
            "layer2.0.relu1\n",
            "layer2.0.conv2\n",
            "layer2.0.bn2\n",
            "layer2.0.downsample\n",
            "layer2.0.downsample.0\n",
            "layer2.0.downsample.1\n",
            "layer2.0.add_relu_FF\n",
            "layer2.0.add_relu_FF.activation_post_process\n",
            "layer2.1\n",
            "layer2.1.conv1\n",
            "layer2.1.bn1\n",
            "layer2.1.relu1\n",
            "layer2.1.conv2\n",
            "layer2.1.bn2\n",
            "layer2.1.add_relu_FF\n",
            "layer2.1.add_relu_FF.activation_post_process\n",
            "layer3\n",
            "layer3.0\n",
            "layer3.0.conv1\n",
            "layer3.0.bn1\n",
            "layer3.0.relu1\n",
            "layer3.0.conv2\n",
            "layer3.0.bn2\n",
            "layer3.0.downsample\n",
            "layer3.0.downsample.0\n",
            "layer3.0.downsample.1\n",
            "layer3.0.add_relu_FF\n",
            "layer3.0.add_relu_FF.activation_post_process\n",
            "layer3.1\n",
            "layer3.1.conv1\n",
            "layer3.1.bn1\n",
            "layer3.1.relu1\n",
            "layer3.1.conv2\n",
            "layer3.1.bn2\n",
            "layer3.1.add_relu_FF\n",
            "layer3.1.add_relu_FF.activation_post_process\n",
            "layer4\n",
            "layer4.0\n",
            "layer4.0.conv1\n",
            "layer4.0.bn1\n",
            "layer4.0.relu1\n",
            "layer4.0.conv2\n",
            "layer4.0.bn2\n",
            "layer4.0.downsample\n",
            "layer4.0.downsample.0\n",
            "layer4.0.downsample.1\n",
            "layer4.0.add_relu_FF\n",
            "layer4.0.add_relu_FF.activation_post_process\n",
            "layer4.1\n",
            "layer4.1.conv1\n",
            "layer4.1.bn1\n",
            "layer4.1.relu1\n",
            "layer4.1.conv2\n",
            "layer4.1.bn2\n",
            "layer4.1.add_relu_FF\n",
            "layer4.1.add_relu_FF.activation_post_process\n",
            "avgpool\n",
            "fc\n",
            "fc._packed_params\n",
            "quant\n",
            "dequant\n"
          ]
        }
      ],
      "source": [
        "for name, module in converted_model.named_modules():\n",
        "  print(name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "for name, module in converted_model.named_modules():\n",
        "    if name == \"fc\":\n",
        "        weights = module.weight().dequantize().flatten().numpy()\n",
        "        plt.hist(weights, bins=2)\n",
        "        plt.title(f\"Histogram of Weights in {name}\")\n",
        "        plt.xlabel(\"Weight value\")\n",
        "        plt.ylabel(\"Frequency\")\n",
        "        plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Direct Pruning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "bucket 0.008137843571603298\n",
            "plunger 0.006934817414730787\n",
            "hook 0.0060799079947173595\n",
            "ladle 0.005567610263824463\n",
            "water jug 0.005421657580882311\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn.utils.prune as prune\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "# Load pretrained ResNet18\n",
        "model = models.resnet18(pretrained=True)\n",
        "\n",
        "# Define pruning amount\n",
        "prune_amount = 0.3  # Prune 30% of weights by magnitude\n",
        "\n",
        "# Apply unstructured pruning to all Conv2d layers\n",
        "for name, module in model.named_modules():\n",
        "    if isinstance(module, nn.Conv2d):\n",
        "        prune.l1_unstructured(module, name='weight', amount=prune_amount)\n",
        "\n",
        "# Optional: Remove pruning reparameterization and make it permanent\n",
        "for name, module in model.named_modules():\n",
        "    if isinstance(module, nn.Conv2d):\n",
        "        prune.remove(module, 'weight')\n",
        "\n",
        "evaluate(model, \"cpu\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model size: 46.83 MB\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "46.828292"
            ]
          },
          "execution_count": 142,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "get_model_size(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Inference time (avg over 50): 15.53 ms\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.015533604621887208"
            ]
          },
          "execution_count": 143,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "measure_inference_time(model)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
