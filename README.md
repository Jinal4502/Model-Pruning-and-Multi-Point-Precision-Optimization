# Model-Pruning-and-Multi-Point-Precision-Optimization
I explored FP16, INT8, mixed-precision quantization, and unstructured pruning on ResNet-18 to study their impact on inference time, size, and accuracy. Since PyTorch pruning isnâ€™t compatible with quantized models, I implemented custom routines to enable effective INT8 and mixed-precision pruning.

# ResNet-18 Quantization and Pruning Analysis

This project investigates the impact of quantization and pruning on the ResNet-18 architecture. We compare model size, inference speed, and top-5 accuracy across four setups:
- INT8 quantization with pruning
- FP16 quantization with pruning
- Mixed-precision (INT8 + FP32 FC layer) with pruning
- Direct unstructured pruning (FP32 baseline)

## ğŸ“ Project Structure

```
mla_project/
â”œâ”€â”€ results/11.png â€“ INT8 FC histogram (pre-pruning)
â”œâ”€â”€ results/12.png â€“ INT8 FC histogram (post-pruning)
â”œâ”€â”€ results/21.png â€“ FP16 FC histogram (pre-pruning)
â”œâ”€â”€ results/22.png â€“ FP16 FC histogram (post-pruning)
â”œâ”€â”€ results/31.png â€“ Mixed FC histogram (pre-pruning)
â”œâ”€â”€ results/32.png â€“ Mixed FC histogram (post-pruning)
â”œâ”€â”€ source_files/mixed_precision.py â€“ Mixed precision quant+prune code
â”œâ”€â”€ source_files/fp16.py â€“ FP16 quant+prune code
â”œâ”€â”€ source_files/direct_pruning_comparison.py â€“ Plain unstructured pruning
â”œâ”€â”€ source_files/int8.py â€“ INT8 quant+prune code
â”œâ”€â”€ resnet.py â€“ Custom ResNet wrapper with fusion support
â”œâ”€â”€ imagenet_classes.txt â€“ Class label mapping
â”œâ”€â”€ n01443537_goldfish.JPEG â€“ Sample image for quick evaluation
â”œâ”€â”€ demo_files/ProjectMilestone3_*_demo.ipynb â€“ Jupyter notebooks for reproducibility
â”œâ”€â”€ Model Pruning and Multi-Point Precision Optimization_UpdatedPresentation - Updated PPT
```

## ğŸ“Œ Requirements

Install dependencies using:

```bash
pip install torch torchvision matplotlib tqdm
```

## ğŸ§ª Usage

Each experiment is run via the respective script:

```bash
python int8.py              # INT8 quantization + pruning
python fp16.py              # FP16 quantization + pruning
python mixed_precision.py   # Mixed precision (INT8 + FP32) + pruning
python direct_pruning_comparison.py   # Plain unstructured pruning
```

Each script will output:
- Inference latency (on CPU and GPU)
- Top-5 accuracy on 500 ImageNet val samples
- Model size in MB
- Histogram plots of FC layer weights (before and after pruning)

## ğŸ“Š Evaluation Summary

| Mode               | Inference Speed (CPU) | Model Size | Top-5 Accuracy |
|--------------------|------------------------|-------------|----------------|
| INT8 + Pruning     | 782 ms                | 11.83 MB    | 76.45%         |
| FP16 + Pruning     | 16.55 ms              | 23.43 MB    | 81.98%         |
| Mixed (INT8+FP32)  | 80.82 ms              | 14.29 MB    | 79.23%         |
| Pruned FP32        | 12.11 ms              | 44.96 MB    | 89.54%         |

## ğŸ“· Visualizations

Each mode includes two plots of the FC layerâ€™s weight distribution:
- **Pre-Pruning**: Weight distribution post-quantization
- **Post-Pruning**: Effect of L1 magnitude pruning

See files: `11.png`â€“`32.png`.

## ğŸ“ Report

The full report (`.pdf`) contains:
- Background on quantization and pruning
- Why PTQ and unstructured pruning were used
- Evaluation metrics and trade-off discussion
- Model architecture outputs and pruning pseudocode

## ğŸ“„ License

This repository is for academic and research use only.
